{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJFZEFq4ueIi"
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "The exercise focuses on backpropagation. We begin with pen and paper tasks followed by code tasks which build on the previous exercise.\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pen and Paper Tasks\n",
    "\n",
    "Perform a forward and backward pass to calculate the gradients for the weights $w_0, w_1, w_2, w_s$ in the following MLP. Each node represents one unit with a weight $w_i, i \\in \\{0, 1, 2\\}$ connecting it to the previous node. The connection from unit 0 to unit 2 is called a __skip connection__, which means unit 2 receives input from two sources and thus has an additional weight $w_s$. The weighted inputs are added before the nonlinearity is applied.\n",
    "\n",
    "**![There should be an image here. If you can't see it, you probably forgot to download the mlp.png!](attachment:image.png)**\n",
    "\n",
    "We assume that we want to solve a regression task. We use an L1-loss $L(\\hat{y}, y) = |y - \\hat{y}|$\n",
    "\n",
    "The nonlinearities for the first two units are rectified linear functions/units (ReLU): $g_0(z) = g_1(z) = \\begin{cases} 0, z<0\\\\ z, else \\end{cases}$.\n",
    "\n",
    "We do not use a nonlinearity for the second unit: $g_2(z_2) = z_2$.\n",
    "\n",
    "**Note:** We use the notation of the Deep Learning book here, i.e. $z = Wx+b$. If you attended the Machine Learning course, you might be used to the different notation used in the Bishop Book, where $z$ denotes the value after applying the activation function. Here, $z$ is the value before applying the activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the backpropagation algorithm for the above network** (2 points)\n",
    "\n",
    "First perform the forward pass, followed by the backward pass to obtain gradients for the weights $w_0, w_1, w_2, w_s$.\n",
    "\n",
    "You can paste a scanned image of your solution to this exercise on paper into this Jupyter notebook. Or if you want to make it easier for the tutors to correct, you can also answer in Latex code in this cell. Some example Latex code is entered here to help you get started.\n",
    "\n",
    "Please assign equations/values to variables and reuse them later when possible to make your solution more readable.\n",
    "\n",
    "#### START TODO ####\n",
    "\n",
    "__Forward Pass:__\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{y} &=& g_2(z_2) \\\\\n",
    "    z_2 &=& w_2 h_1 + w_sh_0 \\\\\n",
    "    h_1 &=& g_1 (z_1) \\\\\n",
    "    z_1 &=& w_1 h_0\\\\\n",
    "    h_0 &=& g_0 (z_0) \\\\\n",
    "    z_0 &=& w_0 x\n",
    "\\end{align*}  \n",
    "\n",
    "\n",
    "__Backward Pass:__\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} =  \\begin{cases} 1 \\quad if \\quad\\hat{y} > y\\\\ -1 \\quad else\\end{cases}\\\\\n",
    "    \\frac{\\partial L}{\\partial z_2} &= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2}\n",
    "    \\\\\n",
    "    \\frac{\\partial L}{\\partial h_1} =\\frac{\\partial L}{\\partial z_2}\\cdot\\frac{\\partial z_2}{\\partial h_1} &= \\frac{\\partial L}{\\partial z_2} \\cdot w_2 \\\\\n",
    "    \\frac{\\partial L}{\\partial z_1} &= \\frac{\\partial L}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_1} \\\\\n",
    "    \\frac{\\partial L}{\\partial h_0} &= \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial h_0}   + \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_0} \\\\ \n",
    "    \\frac{\\partial L}{\\partial z_0} &= \\frac{\\partial L}{\\partial h_0} \\cdot \\frac{\\partial h_0}{\\partial z_0} \\\\\n",
    "    \\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_2} \\\\\n",
    "    \\frac{\\partial L}{\\partial w_s} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_s} \\\\\n",
    "    \\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1} \\\\\n",
    "\\end{align*}   \n",
    "\n",
    "\n",
    "#### END TODO ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What difference does the skip connection make when propagating back the error?** (1 point)\n",
    "\n",
    "#### START TODO ####\n",
    "This solves the vanishing gradient problem. As we progress through the network, the gradients decrease. But when we do a skip connection we preserve the gradient.\n",
    "#### END TODO ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the gradients for the given datapoint and the given initial weights (calculating the gradients requires to calculate a forward pass first). Also calculate the weights and the loss after one gradient descent step.** (3 points)\n",
    "\n",
    "$$(x_1, y_1) = (1, -3) \\\\\n",
    "w_0 = w_1 = w_2 = w_s = 0.5 \\\\\n",
    "Learning Rate = 1 \\\\  $$\n",
    "\n",
    "#### START TODO ####\n",
    "__Forward Pass:__\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{y} &= g_2(z_2) &=& 0.375 \\\\\n",
    "    z_2 &= w_2 h_1 + w_s h_0= 0.5\\cdot0.25 + 0.5\\cdot0.5 &=& 0.375 \\\\\n",
    "    h_1 &= g_1 (z_1) &=& 0.25\\\\\n",
    "    z_1 &= w_1 h_0 = 0.5\\cdot0.5 &=& 0.25\\\\\n",
    "    h_0 &= g_0 (z_0) &=& 0.5\\\\\n",
    "    z_0 &= w_0 x &=& 0.5\n",
    "\\end{align*}  \n",
    "\n",
    "\n",
    "__Backward Pass:__\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} =  \\begin{cases} 1 \\quad if \\quad\\hat{y} > y\\\\ -1 \\quad else\\end{cases} &=& 1\\\\\n",
    "    \\frac{\\partial L}{\\partial z_2} &= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} =1\\cdot 1 &=& 1\n",
    "    \\\\\n",
    "    \\frac{\\partial L}{\\partial h_1} &= \\frac{\\partial L}{\\partial z_2} \\cdot w_2 &=& 0.5\\\\\n",
    "    \\frac{\\partial L}{\\partial z_1} &= \\frac{\\partial L}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_1} = 0.5 \\cdot 1&=& 0.5\\\\\n",
    "    \\frac{\\partial L}{\\partial h_0} &= \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial h_0}   + \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_0} =  0.5 +0.25 &=&0.75 \\\\ \n",
    "    \\frac{\\partial L}{\\partial z_0} &= \\frac{\\partial L}{\\partial h_0} \\cdot \\frac{\\partial h_0}{\\partial z_0} &=&  0.75\\cdot 1\\\\\n",
    "    \\frac{\\partial L}{\\partial w_2} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_2} &=& 0.25\\\\\n",
    "    \\frac{\\partial L}{\\partial w_s} &= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_s} &=& 0.5\\\\\n",
    "    \\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1} = 0.5\\cdot 0.5 &=& 0.25\\\\\n",
    "    \\frac{\\partial L}{\\partial w_0} &= \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_0} = 0.75\\cdot 1 &=& 0.75\\\\\n",
    "\\end{align*}   \n",
    "\n",
    "__One Gradient Descent Step:__\n",
    "\\begin{align*}\n",
    "    (x_1,y_1) &=& (1,3)  \\\\\n",
    "    w_0 = w_1 = w_2 = w_s &=& 0.5 \\\\\n",
    "    Learning Rate &=& 1 \\\\  \n",
    "    w_2 = w_2 - \\lambda \\cdot \\frac{\\partial L}{\\partial w_2}= 0.5 - 1\\cdot 0.25 &=& 0.25 \\\\\n",
    "    w_1 = w_1 - \\lambda \\cdot \\frac{\\partial L}{\\partial w_1}= 0.5 - 1\\cdot 0.25 &=& 0.25 \\\\\n",
    "    w_s = w_s - \\lambda \\cdot \\frac{\\partial L}{\\partial w_s}= 0.5 - 1\\cdot 0.5 &=& 0 \\\\\n",
    "    w_0 = w_0 - \\lambda \\cdot \\frac{\\partial L}{\\partial w_0}= 0.5 - 1\\cdot 0.75 &=& -0.25 \\\\\n",
    "\\end{align*}  \n",
    "\n",
    "\n",
    "__Forward Pass:__\n",
    "\\begin{align*}\n",
    "    L(y,\\hat{y}) &=& 3 \\\\\n",
    "    \\hat{y} = g_2(z_2) &=& 0 \\\\\n",
    "    z_2 = w_2 h_1 + w_s h_0= 0.25\\cdot0 + 0 &=& 0 \\\\\n",
    "    h_1 &=& 0\\\\\n",
    "    z_1 = w_1 h_0 = 0.25\\cdot0 &=& 0.\\\\\n",
    "    h_0 &=& 0\\\\\n",
    "    z_0 = -0.25 \\cdot 0 &=& -0.25\n",
    "\\end{align*}\n",
    "#### END TODO ####\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Tasks\n",
    "\n",
    "The coding tasks build on the previous exercise. We complete implementing a small feedforward neural network and then use backprop to obtain gradients and update the weights of the network for the XOR dataset as last time. We provide code for structure and utility and you have to **fill in the TODO-gaps**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YIChkVlueIl"
   },
   "outputs": [],
   "source": [
    "# Some imports used in the code below\n",
    "from typing import Iterable, List, Optional, Tuple  # type annotations\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "import scipy.optimize  # gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2w5-mnoueIu"
   },
   "source": [
    "In the previous exercise, you used the **Parameter** and **Module** classes and implemented their forward passes. This time you will implement their backward passes.\n",
    "\n",
    "*Note:* An additional utility module is used to check the correctness of your implementation by approximating *backward* with [finite difference approximations](https://en.wikipedia.org/wiki/Finite_difference#Relation_with_derivatives)  of *forward*. All modules operate on batches of samples. E.g. the input shape of `Linear.forward` is `(batch_size, feature_shape, 1)` (we will use the last dimension in future exercises)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8f1UWYwFueIw"
   },
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    \"\"\"A trainable parameter.\n",
    "\n",
    "    This class not only stores the value of the parameter (self.data) but also tensors/\n",
    "    properties associated with it, such as the gradient (self.grad) of the current backward\n",
    "    pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, grad: Optional[np.ndarray] = None, name=None):\n",
    "        self.data = data  # type: np.ndarray\n",
    "        self.grad = grad  # type: Optional[np.ndarray]\n",
    "        self.name = name  # type: Optional[str]\n",
    "        self.state_dict = dict()  # dict to store additional, optional information\n",
    "        \n",
    "        \n",
    "class Module:\n",
    "    \"\"\"The base class all network modules must inherit from.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Cache of the input of the forward pass.\n",
    "        # We need it during the backward pass in most layers,\n",
    "        #  e.g., to compute the gradient w.r.t to the weights.\n",
    "        self.input_cache = None\n",
    "\n",
    "    def __call__(self, *args) -> np.ndarray:\n",
    "        \"\"\"Alias for forward, convenience function.\"\"\"\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def forward(self, *args) -> np.ndarray:\n",
    "        \"\"\"Compute the forward pass through the module.\n",
    "\n",
    "        Args:\n",
    "           args: The inputs, e.g., the output of the previous layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the backward pass through the module.\n",
    "\n",
    "        This method computes the gradients with respect to the trainable\n",
    "        parameters and with respect to the first input.\n",
    "        If the module has trainable parameters, this method needs to update\n",
    "        the respective parameter.grad property.\n",
    "\n",
    "        Args:\n",
    "            grad: The gradient of the following layer.\n",
    "\n",
    "        Returns:\n",
    "            The gradient with respect to the first input argument. In general\n",
    "            it might be useful to return the gradients w.r.t. to all inputs, we\n",
    "            omit this here to keep things simple.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        \"\"\"Return the module parameters.\"\"\"\n",
    "        return []  # default to empty list\n",
    "\n",
    "    def check_gradients(self, input_args: Tuple[np.ndarray]):\n",
    "        \"\"\"Verify the implementation of the gradients.\n",
    "\n",
    "        This includes the gradient with respect to the input as well as the\n",
    "        gradients w.r.t. the parameters if the module contains any.\n",
    "\n",
    "        As the scipy grad check only works on scalar functions, we compute\n",
    "        the sum over the output to obtain a scalar.\n",
    "        \"\"\"\n",
    "        assert isinstance(input_args, tuple), (\n",
    "            \"input_args must be a tuple but is {}\".format(type(input_args)))\n",
    "        TOLERANCE = 1e-6\n",
    "        self.check_gradients_wrt_input(input_args, TOLERANCE)\n",
    "        self.check_gradients_wrt_params(input_args, TOLERANCE)\n",
    "\n",
    "    def _zero_grad(self):\n",
    "        \"\"\"(Re-) intialize the param's grads to 0. Helper for grad checking.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def check_gradients_wrt_input(self, input_args: Tuple[np.ndarray],\n",
    "                                  tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. input.\"\"\"\n",
    "\n",
    "        def output_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.forward for scipy.optimize.check_grad.\"\"\"\n",
    "            # we only compute the gradient w.r.t. to the first input arg.\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            return np.sum(self.forward(*args))\n",
    "\n",
    "        def grad_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.backward for scipy.optimize.check_grad.\"\"\"\n",
    "            self._zero_grad()\n",
    "            # run self.forward to store the new input\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            out = self.forward(*args)\n",
    "            # compute the gradient w.r.t. to the input\n",
    "            return np.ravel(self.backward(np.ones_like(out)))\n",
    "\n",
    "        error = scipy.optimize.check_grad(\n",
    "            output_given_input, grad_given_input, np.ravel(input_args[0]))\n",
    "        num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "        if np.squeeze(error) / num_outputs > tolerance:\n",
    "            raise RuntimeError(\"Check of gradient w.r.t. to input for {} failed.\"\n",
    "                               \"Error {:.4E} > {:.4E}.\"\n",
    "                               .format(self, np.squeeze(error), tolerance))\n",
    "\n",
    "    def check_gradients_wrt_params(self, input_args: Tuple[np.ndarray],\n",
    "                                   tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. params.\"\"\"\n",
    "        for param in self.parameters():\n",
    "            def output_given_params(new_param: np.ndarray):\n",
    "                \"\"\"Wrap self.forward, change the parameters to new_param.\"\"\"\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                return np.sum(self.forward(*input_args))\n",
    "\n",
    "            def grad_given_params(new_param: np.ndarray):\n",
    "                self._zero_grad()\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                out = self.forward(*input_args)\n",
    "                # compute the gradient w.r.t. to param\n",
    "                self.backward(np.ones_like(out))\n",
    "                return np.ravel(param.grad)\n",
    "            # flatten the param as scipy can only handle 1D params\n",
    "            param_init = np.ravel(np.copy(param.data))\n",
    "            error = scipy.optimize.check_grad(output_given_params,\n",
    "                                              grad_given_params,\n",
    "                                              param_init)\n",
    "            num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "            if np.squeeze(error) / num_outputs > tolerance:\n",
    "                raise RuntimeError(\"Check of gradient w.r.t. to param '{}' for\"\n",
    "                                   \"{} failed. Error {:.4E} > {:.4E}.\"\n",
    "                                   .format(param.name, self, error, tolerance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pvmc_mniueI2"
   },
   "source": [
    "# Nonlinearities\n",
    "\n",
    "Implement the backward passes for the *Module*s below. The *backward()* function of a *Module* receives a *grad* argument from the *Module* after it in the network and using the chain rule calculates the gradient to be passed to *Module*s before it. This way the gradient information flows backward through backpropagation. The sigmoid is already solved for you as an example.\n",
    "\n",
    "## Sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLb-27dFueI4"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def _sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        assert len(z.shape) == 3, (\"z.shape should be (batch_size, input_size, 1)\"\n",
    "                                   \" but is {}.\".format(z.shape))\n",
    "        h = self._sigmoid(z)\n",
    "        # here it's useful to store the activation \n",
    "        #  instead of the input\n",
    "        self.input_cache = h\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        h = self.input_cache\n",
    "        return grad * h * (1 - h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMQ5DmToueJC"
   },
   "source": [
    "## Relu (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBYjj761ueJD"
   },
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = z\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        z = self.input_cache\n",
    "        # START TODO ################\n",
    "        #raise NotImplementedError\n",
    "        grad.reshape(z.shape)\n",
    "        return grad*np.where(z>=0,1,0)\n",
    "        # END TODO###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zsNa01IueJN"
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWZf9BhTueJP"
   },
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    def _softmax(self, z):\n",
    "        # don't reduce (sum) over batch axis\n",
    "        reduction_axes = tuple(range(1, len(z.shape))) \n",
    "        \n",
    "        # Shift input for numerical stability.\n",
    "        shift_z = z - np.max(z, axis=reduction_axes, keepdims=True)\n",
    "        exps = np.exp(shift_z)\n",
    "        h = exps / np.sum(exps, axis=reduction_axes, keepdims=True)\n",
    "        return h\n",
    "    \n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        h = self._softmax(z)\n",
    "        return h\n",
    "\n",
    "    def backward(self, grad) -> np.ndarray:\n",
    "        error_msg = (\"Softmax doesn't need to implement a gradient here, as it's\"\n",
    "                     \"only needed in CrossEntropyLoss, where we can simplify\"\n",
    "                     \"the gradient for the combined expression.\")\n",
    "        raise NotImplementedError(error_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuHOIiLhueJX"
   },
   "source": [
    "# Linear Layer (2 points)\n",
    "\n",
    "Implement the backward pass for the Linear layer Module. Remember that the Linear layer holds objects of the Parameter class and you would need to update their gradients during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "inuuMNmUueJY"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        w_data = 0.5 * np.random.randn(out_features, in_features)\n",
    "        self.W = Parameter(w_data, None, \"W\")\n",
    "        \n",
    "        b_data = 0.01 * np.ones((out_features, 1))\n",
    "        self.b = Parameter(b_data, None, \"b\")\n",
    "        \n",
    "        self._zero_grad()\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(x.shape) == 3, (\"x.shape should be (batch_size, input_size, 1)\"\n",
    "                                   \" but is {}.\".format(x.shape))\n",
    "        self.input_cache = x\n",
    "        # Remember: Access weight data through self.W.data\n",
    "        z = self.W.data @ x + self.b.data\n",
    "        return z\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        # Return all parameters of Linear\n",
    "        return self.W, self.b\n",
    "        \n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        x = self.input_cache\n",
    "        # remember that input has a batch dimension when transposing, i.e.,\n",
    "        # we need to use np.transpose instead of x.T\n",
    "        x_transpose = np.transpose(x, [0, 2, 1])\n",
    "        \n",
    "        # START TODO ################ \n",
    "        # self.W.grad += ...\n",
    "        # ...\n",
    "        self.W.grad += np.sum(grad@x_transpose,axis=0)\n",
    "        self.b.grad += np.sum(grad,axis=0)\n",
    "        \n",
    "        #raise NotImplementedError\n",
    "        return np.transpose(self.W.data) @ grad\n",
    "        # END TODO ##################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8FtzDUfueJd"
   },
   "source": [
    "# Cost Function\n",
    "\n",
    "## Cross Entropy (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xezzvQYTueJe"
   },
   "outputs": [],
   "source": [
    "# Define the Cross-Entropy cost functions\n",
    "class CrossEntropyLoss(Module):\n",
    "    \"\"\"Compute the cross entropy.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, a: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the cross entropy, mean over batch size.\"\"\"\n",
    "        a = self.softmax(a)\n",
    "        self.input_cache = a, y\n",
    "        # compute the mean over the batch\n",
    "        return -np.sum(np.log(a[y == 1])) / len(a)\n",
    "\n",
    "    def backward(self, _=None) -> np.ndarray:\n",
    "        # we introduce the argument _ here, to have a unified interface with\n",
    "        # other Module objects. This simplifies code for gradient checking. \n",
    "        # We don't need this arg.\n",
    "        a, y = self.input_cache\n",
    "        \n",
    "        # START TODO ################ \n",
    "        #raise NotImplementedError\n",
    "        grad = (a-y)/a.shape[0]\n",
    "        #print('grad shape',grad.shape)\n",
    "        #print(\"a-y\",(a-y).shape)\n",
    "        # END TODO ##################\n",
    "\n",
    "        # Recreate the batch dimension\n",
    "        grad = np.expand_dims(grad, -1)\n",
    "        \n",
    "        assert len(grad.shape) == 3, (\"CrossEntropyLoss.backward should return (batch_size, grad_size, 1)\"\n",
    "                                      \" but is {}.\".format(grad.shape))\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzJ8oxApueJi"
   },
   "source": [
    "# Sequential Network (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14vfdOnGueJk"
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"A sequential container to stack modules.\n",
    "\n",
    "    Modules will be added to it in the order they are passed to the\n",
    "    constructor.\n",
    "\n",
    "    Example network with one hidden layer:\n",
    "    model = Sequential(\n",
    "                  Linear(5,10),\n",
    "                  ReLU(),\n",
    "                  Linear(10,10),\n",
    "                )\n",
    "    \"\"\"\n",
    "    def __init__(self, *args: List[Module]):\n",
    "        super().__init__()\n",
    "        self.modules = args\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Remember: module(x) is equivalent to module.forward(x)\n",
    "        for module in self.modules:\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "        # Perform the backward pass in reverse of the order that the Modules were present in the args to\n",
    "        # the Network during its initialization. Python provieds a utility reversed() in order to reverse a list.\n",
    "        #raise NotImplementedError\n",
    "        for module in reversed(self.modules):\n",
    "            grad = module.backward(grad)\n",
    "        # END TODO ##################\n",
    "        return grad\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        # iterate over modules and retrieve their parameters, iterate over\n",
    "        # parameters to flatten the list\n",
    "        return [param for module in self.modules\n",
    "                for param in module.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding\n",
    "Although binary classification for XOR can be done without using one hot encdoing, here we will be using the Softmax which allows us to be more general and perform multi-class classification of which binary classification is a special case. To handle categorical data of multiple classes, we will use the one_hot_encoding utility function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"Convert integer labels to one hot encoding.\n",
    "\n",
    "    Example: y=[1, 2] --> [[0, 1, 0], [0, 0, 1]]\n",
    "    \"\"\"\n",
    "    encoded = np.zeros(y.shape + (num_classes,))\n",
    "    encoded[np.arange(len(y)), y] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "y = np.array([1, 2, 0])\n",
    "np.testing.assert_equal(one_hot_encoding(y, 3), [[0, 1, 0], [0, 0, 1], [1, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Check (1 point)\n",
    "\n",
    "Gradient checking is a useful utility to check, whether gradients obtained through finite differences and backward pass are matching. We have implemented the gradient checking in the Module class for you. As all classes we defined up to here inherit from Module, we can run `check_gradients` to check if you have implemented their backward passes correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = np.random.uniform(-1., 1., size=(2, 10, 1))\n",
    "input_args = (input_vector,)\n",
    "\n",
    "# layers + activations\n",
    "Relu().check_gradients(input_args)\n",
    "Sigmoid().check_gradients(input_args)\n",
    "Linear(10, 30).check_gradients(input_args)\n",
    "\n",
    "# START TODO ################ \n",
    "# Instantiate a Sequential network with layers: linear, sigmoid, linear and \n",
    "# perform the gradient check on it.\n",
    "model = Sequential(\n",
    "                  Linear(10,30),\n",
    "                  Sigmoid(),\n",
    "                  Linear(30,10),\n",
    "                )\n",
    "model.check_gradients(input_args)\n",
    "\n",
    "#raise NotImplementedError\n",
    "# END TODO ##################\n",
    "\n",
    "# losses\n",
    "input_args_losses = (one_hot_encoding(np.array([1, 2]), 3),  # a\n",
    "                     one_hot_encoding(np.array([1, 1]), 3))  # y (ground truth)\n",
    "CrossEntropyLoss().check_gradients(input_args_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmNlenRuueKF"
   },
   "source": [
    "# Experiments (2 points)\n",
    "\n",
    "We use the XOR dataset from last time and perform backpropagation on the 1 hidden layer model from last time, except that we now use 2 output units to perform binary classification as multi-class classification. First, perform 1 step of backprop and use the gradient you obtain to update the weights of the network. See how the parameters and their gradients change in the network. Then perform mutiple steps and see how the loss on the dataset evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-YiK7YfHgXl"
   },
   "outputs": [],
   "source": [
    "# training set\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# true labels\n",
    "Y = np.array([0, 1, 1, 0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E71BjGgsKJOI",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial predictions [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Loss before any steps taken: 0.7041332661110266\n",
      "Params before:\n",
      "W\n",
      "[[ 0.43640713  0.13478894]\n",
      " [-0.74745996 -0.0280766 ]]\n",
      "b\n",
      "[[0.01]\n",
      " [0.01]]\n",
      "W\n",
      "[[-0.69947647  0.17088238]\n",
      " [ 0.1357607  -0.69550366]]\n",
      "b\n",
      "[[0.01]\n",
      " [0.01]]\n",
      "Grad before:\n",
      "W\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "b\n",
      "[[0.]\n",
      " [0.]]\n",
      "W\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "b\n",
      "[[0.]\n",
      " [0.]]\n",
      "Grad after:\n",
      "W\n",
      "[[0.04409628 0.03116005]\n",
      " [0.         0.        ]]\n",
      "b\n",
      "[[ 0.05038533]\n",
      " [-0.10828139]]\n",
      "W\n",
      "[[-0.02867187 -0.00124981]\n",
      " [ 0.02867187  0.00124981]]\n",
      "b\n",
      "[[-0.06032457]\n",
      " [ 0.06032457]]\n",
      "Params after:\n",
      "W\n",
      "[[ 0.39231085  0.1036289 ]\n",
      " [-0.74745996 -0.0280766 ]]\n",
      "b\n",
      "[[-0.04038533]\n",
      " [ 0.11828139]]\n",
      "W\n",
      "[[-0.67080459  0.17213219]\n",
      " [ 0.10708883 -0.69675346]]\n",
      "b\n",
      "[[ 0.07032457]\n",
      " [-0.05032457]]\n",
      "Loss after 1 step: 0.698717356371839\n",
      "Loss after 2 steps: 0.6853553636114985\n",
      "Loss after 3 steps: 0.6776943431805246\n",
      "Loss after 4 steps: 0.6670292156283242\n",
      "Loss after 5 steps: 0.66752554037043\n",
      "Loss after 6 steps: 0.6549738303836905\n",
      "Loss after 7 steps: 0.6502977791820745\n",
      "Loss after 8 steps: 0.6425756106224867\n",
      "Loss after 9 steps: 0.6264513930791764\n",
      "Loss after 10 steps: 0.626152460500505\n",
      "Loss after 11 steps: 0.613147221793812\n",
      "Loss after 12 steps: 0.5945429664211289\n",
      "Loss after 13 steps: 0.5993611066268242\n",
      "Loss after 14 steps: 0.6073877553641905\n",
      "Loss after 15 steps: 0.582745293650599\n",
      "Loss after 16 steps: 0.5637450901906467\n",
      "Loss after 17 steps: 0.5475558059345079\n",
      "Loss after 18 steps: 0.5581516229880139\n",
      "Loss after 19 steps: 0.5734118502178327\n",
      "Loss after 20 steps: 0.5461034991991756\n",
      "Loss after 21 steps: 0.5289130447619375\n",
      "Loss after 22 steps: 0.5153468398136812\n",
      "Loss after 23 steps: 0.5039506399409026\n",
      "Loss after 24 steps: 0.4974200490034748\n",
      "Loss after 25 steps: 0.5112725852987412\n",
      "Loss after 26 steps: 0.5251754177198011\n",
      "Loss after 27 steps: 0.5011813865455297\n",
      "Loss after 28 steps: 0.4963238657385351\n",
      "Loss after 29 steps: 0.482602336872519\n",
      "Loss after 30 steps: 0.4719189700597721\n",
      "Loss after 31 steps: 0.46816481660700604\n",
      "Loss after 32 steps: 0.4677574426180272\n",
      "Loss after 33 steps: 0.45073427928374654\n",
      "Loss after 34 steps: 0.44742577125090477\n",
      "Loss after 35 steps: 0.4519047669500662\n",
      "Loss after 36 steps: 0.43334237231885286\n",
      "Loss after 37 steps: 0.43840573420670415\n",
      "Loss after 38 steps: 0.43511620509027726\n",
      "Loss after 39 steps: 0.4132618689837701\n",
      "Loss after 40 steps: 0.4309334599125514\n",
      "Loss after 41 steps: 0.40143525440639605\n",
      "Loss after 42 steps: 0.41996123030475346\n",
      "Loss after 43 steps: 0.38914018384404525\n",
      "Loss after 44 steps: 0.40143671165829276\n",
      "Loss after 45 steps: 0.3735176812319192\n",
      "Loss after 46 steps: 0.3832837736448901\n",
      "Loss after 47 steps: 0.36247331725341375\n",
      "Loss after 48 steps: 0.3635687553725934\n",
      "Loss after 49 steps: 0.3614600343022649\n",
      "Loss after 50 steps: 0.34818498078039106\n",
      "Loss after 51 steps: 0.35758337928629497\n",
      "Loss after 52 steps: 0.32433809472690106\n",
      "Loss after 53 steps: 0.3402474599742933\n",
      "Loss after 54 steps: 0.2985336019283618\n",
      "Loss after 55 steps: 0.338995949082209\n",
      "Loss after 56 steps: 0.46888416727032356\n",
      "Loss after 57 steps: 0.5460025182791087\n",
      "Loss after 58 steps: 0.2682171484408816\n",
      "Loss after 59 steps: 0.2677142642358078\n",
      "Loss after 60 steps: 0.2551668192344329\n",
      "Loss after 61 steps: 0.26984038908999264\n",
      "Loss after 62 steps: 0.2238618805695703\n",
      "Loss after 63 steps: 0.25319753124854505\n",
      "Loss after 64 steps: 0.1951356621692893\n",
      "Loss after 65 steps: 0.23317706501644886\n",
      "Loss after 66 steps: 0.16766278503311155\n",
      "Loss after 67 steps: 0.20925717925836212\n",
      "Loss after 68 steps: 0.14190239825941145\n",
      "Loss after 69 steps: 0.1891907033969541\n",
      "Loss after 70 steps: 0.1945096379382003\n",
      "Loss after 71 steps: 0.251726934615687\n",
      "Loss after 72 steps: 0.14567747651542579\n",
      "Loss after 73 steps: 0.17555022587073013\n",
      "Loss after 74 steps: 0.10967511031602106\n",
      "Loss after 75 steps: 0.11927179568199317\n",
      "Loss after 76 steps: 0.14262403624849984\n",
      "Loss after 77 steps: 0.09406141959478442\n",
      "Loss after 78 steps: 0.10056568608097911\n",
      "Loss after 79 steps: 0.11924115987079809\n",
      "Loss after 80 steps: 0.08223338973851141\n",
      "Loss after 81 steps: 0.0866978752103017\n",
      "Loss after 82 steps: 0.09975066285270234\n",
      "Loss after 83 steps: 0.07579446480722973\n",
      "Loss after 84 steps: 0.07244626354100281\n",
      "Loss after 85 steps: 0.08864192025885423\n",
      "Loss after 86 steps: 0.07769863015611472\n",
      "Loss after 87 steps: 0.0657275686931022\n",
      "Loss after 88 steps: 0.07813472692856857\n",
      "Loss after 89 steps: 0.06151033030471706\n",
      "Loss after 90 steps: 0.0645866364864663\n",
      "Loss after 91 steps: 0.07176735878696347\n",
      "Loss after 92 steps: 0.05515719430533277\n",
      "Loss after 93 steps: 0.056826386709129914\n",
      "Loss after 94 steps: 0.06301258160623122\n",
      "Loss after 95 steps: 0.050037553551865205\n",
      "Loss after 96 steps: 0.05166144671498905\n",
      "Loss after 97 steps: 0.056349429506824596\n",
      "Loss after 98 steps: 0.04721631733741871\n",
      "Loss after 99 steps: 0.04568277984518784\n",
      "Loss after 100 steps: 0.051930864414059585\n",
      "Loss after 101 steps: 0.04645951062188222\n",
      "Final predictions [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Define the model here!\n",
    "linear_units = 2\n",
    "model = Sequential(Linear(2, linear_units),\n",
    "                   Relu(),\n",
    "                   Linear(linear_units, 2),\n",
    "                   )\n",
    "\n",
    "# Implement a function to reset the gradients of parameters\n",
    "def zero_grad(params) -> None:\n",
    "    \"\"\"Clear the gradients of all optimized parameters.\"\"\"\n",
    "# START TODO ################\n",
    "    for parm in params:\n",
    "        parm.grad = 0\n",
    "# END TODO ##################\n",
    "\n",
    "lr = 1\n",
    "\n",
    "x = np.expand_dims(X, -1)\n",
    "y = one_hot_encoding(Y, 2)\n",
    "\n",
    "y_predicted = model(x)\n",
    "print(\"Initial predictions\", np.argmax(y_predicted, axis=1))\n",
    "h_1 = np.squeeze(y_predicted)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "loss = loss_fn(h_1, y)\n",
    "print(\"Loss before any steps taken:\", loss)\n",
    "print(\"Params before:\")\n",
    "for p in model.parameters():\n",
    "    print(p.name)\n",
    "    print(p.data)\n",
    "print(\"Grad before:\")\n",
    "for p in model.parameters():\n",
    "    print(p.name)\n",
    "    print(p.grad)\n",
    "# START TODO ################\n",
    "# Perform a backward pass on it to obtain the gradients and then use them to update the parameters of the network\n",
    "grad = loss_fn.backward()\n",
    "model.backward(grad)\n",
    "for p in model.parameters():\n",
    "    p.data = p.data - p.grad * lr\n",
    "# END TODO ##################\n",
    "\n",
    "print(\"Grad after:\")\n",
    "for p in model.parameters():\n",
    "    print(p.name)\n",
    "    print(p.grad)\n",
    "print(\"Params after:\")\n",
    "for p in model.parameters():\n",
    "    print(p.name)\n",
    "    print(p.data)\n",
    "y_predicted = model(x)\n",
    "h_1 = np.squeeze(y_predicted)\n",
    "loss = loss_fn(h_1, y)\n",
    "print(\"Loss after 1 step:\", loss)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    # START TODO ################\n",
    "    # Now perform multiple steps of updating the parameters and observe the loss\n",
    "    # Code will be very similar to single step above but with one very significant change\n",
    "    # END TODO ##################\n",
    "    grad = loss_fn.backward()\n",
    "    model.backward(grad)\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data - p.grad * lr\n",
    "    y_predicted = model(x)\n",
    "    h_1 = np.squeeze(y_predicted)\n",
    "    loss = loss_fn(h_1, y)\n",
    "    print(\"Loss after\", i + 2, \"steps:\", loss)\n",
    "    zero_grad(model.parameters())\n",
    "\n",
    "\n",
    "# Print final predictions after multiple steps of updates\n",
    "y_predicted = model(x)\n",
    "print(\"Final predictions\", np.argmax(y_predicted, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions on experiments (2 points)\n",
    "\n",
    "**What do you observe after 1 step of updating parameters?**\n",
    "\n",
    "#### START TODO ####\n",
    "**Answer:**\n",
    "The loss decreased but with a very small number.\n",
    "#### END TODO ####\n",
    "\n",
    "**What do you observe after multiple steps of updating parameters? Does the loss always decrease? Explain why it may not.**\n",
    "\n",
    "#### START TODO ####\n",
    "**Answer:**\n",
    "The loss decreased until it reached zero. No the loss will not always decreas as there're steps where the loss increased on them and that's because when we update the parameter we subtract the gradient multiplied by learning rate from current value and this may lead to a large gradient step and we miss the optimal point.\n",
    "#### END TODO ####\n",
    "\n",
    "**Run the experiment multiple times. Do you always end up with the correct final predictions? Explain why or why not?**\n",
    "\n",
    "#### START TODO ####\n",
    "**Answer:**\n",
    "No, because every time we start with a different weight so, some weights with the current learning rate will converge after 100 steps and some will be still far away from the minimal point. The problem is non-convex because of the non-linearities in the sigmoid. Therefore, we might get stuck in local minima. Since the initialization of the weights is random, it will affect whether we reach the global minimum or we are stuck in local minima.\n",
    "#### END TODO ####\n",
    "\n",
    "**What is the role of the variable lr above? When would you set it to a relatively larger value and when would you set it to a relatively smaller value? Explain.**\n",
    "\n",
    "#### START TODO ####\n",
    "**Answer:**\n",
    "It controls the step size of the gradient and it should be a large value when we're far from the optimal value and it should be smaller value when we're close to the optimal value.\n",
    "#### END TODO ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzidWhY_Ekqb"
   },
   "source": [
    "(BONUS): After transforming the input data into the hidden representation space, generate two plots showing the dataset in the input and representation spaces, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsWaa2fnntSy"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x1: np.ndarray, x2: np.ndarray, h1: np.ndarray, h2: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    x1: np.ndarray with shape (nr_examples,). First input features.\n",
    "    x2: np.ndarray with shape (nr_examples,). Second input feature.\n",
    "    h1: np.ndarray with shape (nr_examples,). First learned features.\n",
    "    h2: np.ndarray with shape (nr_examples,). Second learned feature.\n",
    "    y: np.ndarray with shape (nr_examples,). True labels.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    for k, (i, j) in enumerate(zip(x1, x2)):\n",
    "        ax[0].scatter(i, j, c='b', marker=r\"${}$\".format(y[k]), s=100)\n",
    "    for k, (i, j) in enumerate(zip(h1, h2)):\n",
    "        ax[1].scatter(i, j, c='b', marker=r\"${}$\".format(y[k]), s=100)\n",
    "\n",
    "    ax[0].set_xlabel('x1')\n",
    "    ax[0].set_ylabel('x2')\n",
    "    ax[0].set_title(\"Original x space\")\n",
    "\n",
    "    ax[1].set_xlabel('h1')\n",
    "    ax[1].set_ylabel('h2')\n",
    "    ax[1].set_title(\"Learned h space\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1572439387939,
     "user": {
      "displayName": "Arber Zela",
      "photoUrl": "https://lh3.googleusercontent.com/-S819W68olkc/AAAAAAAAAAI/AAAAAAAAAhw/gBWxwn5CwRE/s64/photo.jpg",
      "userId": "10640764346134349069"
     },
     "user_tz": -60
    },
    "id": "r5kvCHpNDklm",
    "outputId": "de635a47-d288-4152-ed54-156ad639990b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_hidden(full_model: Module, x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Function to extraxt the hidden representation from a MLP.\n",
    "    Args:\n",
    "    full_model: Module. The sequential model used as a classifier\n",
    "    x: np.ndarray with shape (nr_examples, nr_features). Input examples\n",
    "    Returns:\n",
    "    h: np.ndarray with shape (nr_examples, nr_features). Hidden representation of inputs.\n",
    "    \"\"\"\n",
    "    # Extract the hidden features from the sequential model defined above and\n",
    "    # compute the hidden representation after propagating the input through\n",
    "    # the first Linear layer and the ReLU function.\n",
    "    h = Sequential(*full_model.modules[:-1])\n",
    "    h = h(np.expand_dims(x, -1)).reshape(4, 2)\n",
    "    return h\n",
    "\n",
    "h = extract_hidden(model, X)\n",
    "plot(X[:, 0], X[:, 1], h[:, 0], h[:, 1], Y.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-atiph1GueKF"
   },
   "source": [
    "** Your feedback on exercise 3: ** \n",
    "- Do we have to use zero_grad function in each iteration when we try to do multiple steps? before using it we got the loss equal to zero faster after only 50 iterations or something and even when we run it multiple times it get faster to zero loss than now when we use zero_grad function after each iteration. \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "name": "exercise02_python_mlp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
