{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "ex8_ConNet_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHw4RCWQ-snU",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 8\n",
        "This exercise focuses on CNN architectures. We'll continue using Pytorch.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Implement basic architectures.\n",
        "- Experiment with architectures, hyperparameters and batch normalization.\n",
        "- Design and implement a Convolutional network that improves classification accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZN2xtvB-snX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8I0gvih-sna",
        "colab_type": "text"
      },
      "source": [
        "### Loading and normalizing the CIFAR10 dataset\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 colour images in 10 classes, 50,000 training images and 10,000 test images of size 32 x 32.\n",
        "\n",
        "https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "\n",
        "The pytorch provides a package called torchvision, that automatically download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsBBtitY-snc",
        "colab_type": "code",
        "outputId": "6be5e4f7-4b20-4949-db98-88dcd14e979f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:06, 27098928.56it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du9mTki6-sne",
        "colab_type": "code",
        "outputId": "6fdf0140-63ed-42d1-84ab-b1d2330d21e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "img = torchvision.utils.make_grid(images[:4,:,:,:])\n",
        "\n",
        "# show images\n",
        "img = img / 2 + 0.5     # unnormalize\n",
        "npimg = img.numpy()\n",
        "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "plt.show()\n",
        "\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAB5CAYAAAAgYXpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPDwmFP4B7uHh4TGh8A9wDw8PjwnF/wd3nVrKtH4G6wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " deer  deer plane   car\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXowBdM--snh",
        "colab_type": "text"
      },
      "source": [
        "### Define a Convolutional Neural Network\n",
        "\n",
        "A simple Convolutional Network is a sequence of layers. The most used layers to build ConvNets are Convolutional Layer, Pooling Layer, and Fully-Connected Layer.\n",
        "\n",
        "Your job is to stack the three main types of layers to form a full ConvNet architecture following the pattern in CIFAR-10:\n",
        "\n",
        " [INPUT - CONV - RELU - POOL - FC]\n",
        " \n",
        "* INPUT [32x32x3]: the RGB raw pixel values of the image.\n",
        "* CONV Convolutional layer with 3 filters and filter size 5 (use padding value 2 to preserve the input dimension).\n",
        "* RELU applies an elementwise activation function.\n",
        "* POOL downsampling operation through the spatial dimensions kernel size = 2.\n",
        "* FC fully-connected layer that computes the class scores, with output size [1x1x10], where each of the 10 numbers corresponds to a class score. Each neuron in this layer will be connected to all the numbers in the previous volume.\n",
        "\n",
        "Implement the `forward` function, remember that the `backward` pass is computed automatically in Pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZPCMOYJ-snh",
        "colab_type": "code",
        "outputId": "2dcc771e-f9af-4a09-bc30-0940294f3c0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "Nfilters = 3\n",
        "Ksize = 5\n",
        "padding = 2\n",
        "class ConvNet1(nn.Module):\n",
        "    \"\"\"\n",
        "        The CNN convolutional network with architecture defined above\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # START TODO #############\n",
        "        # initialize required parameters / layers needed to build the network\n",
        "        self.conv1 = nn.Conv2d(3, Nfilters, kernel_size= Ksize , stride=1 , padding=padding)                           \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(768, 10)\n",
        "        \n",
        "        # END TODO #############\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        \n",
        "        Args:\n",
        "            x: The input tensor with shape [batch_size, feature_dim] (minibatch of data)\n",
        "        Returns:\n",
        "            scores: Pytorch tensor of shape (N, C) giving classification scores for x\n",
        "        \"\"\"\n",
        "        # START TODO #############\n",
        "        \n",
        "        # Remember to flatten the feature map using:\n",
        "        # x = x.view(batch_size, dim)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        # END TODO #############\n",
        "\n",
        "        return x\n",
        "\n",
        "net = ConvNet1()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet1(\n",
            "  (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=768, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oE6pvO--snj",
        "colab_type": "text"
      },
      "source": [
        "Now let's train and see the result of the network. If your implementation is correct, you should get around 47% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEHu9oEa-snk",
        "colab_type": "code",
        "outputId": "46512c29-0f3f-4f76-b120-317f4d72f6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "# Define a Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(2):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.007\n",
            "[1,  2000] loss: 0.883\n",
            "[1,  3000] loss: 0.852\n",
            "[1,  4000] loss: 0.832\n",
            "[1,  5000] loss: 0.825\n",
            "[1,  6000] loss: 0.819\n",
            "[1,  7000] loss: 0.799\n",
            "[1,  8000] loss: 0.799\n",
            "[1,  9000] loss: 0.801\n",
            "[1, 10000] loss: 0.790\n",
            "[1, 11000] loss: 0.773\n",
            "[1, 12000] loss: 0.788\n",
            "[2,  1000] loss: 0.741\n",
            "[2,  2000] loss: 0.753\n",
            "[2,  3000] loss: 0.760\n",
            "[2,  4000] loss: 0.750\n",
            "[2,  5000] loss: 0.755\n",
            "[2,  6000] loss: 0.750\n",
            "[2,  7000] loss: 0.747\n",
            "[2,  8000] loss: 0.769\n",
            "[2,  9000] loss: 0.749\n",
            "[2, 10000] loss: 0.752\n",
            "[2, 11000] loss: 0.756\n",
            "[2, 12000] loss: 0.756\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 49 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt1mB2WC-snn",
        "colab_type": "text"
      },
      "source": [
        "### Changing the parameters of the network - Number of filters\n",
        "\n",
        "Use the previous ConvNet and change the number of filters used in the convolutional layer.\n",
        "\n",
        "(16 or 32 are good options to try!)\n",
        "\n",
        "Describe what happens when you change the number of filters. Do more or fewer do better?\n",
        "\n",
        "**Answer**:  \n",
        "**TODO**  \n",
        "The accuracy improved by 12%. Increasing the number of filters improved the performance. Using a larger number of filters results in a more powerful model, however we face an increased risk of overfitting. Therefore, it is advisable to start with small number and then increase it step by step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFiV5jco-sno",
        "colab_type": "code",
        "outputId": "eb382d93-225e-4e76-ae64-80aa0b2fac1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "Nfilters = 16 # also try 32\n",
        "Ksize = 5\n",
        "padding = 2\n",
        "class ConvNet2(nn.Module):\n",
        "    \"\"\"\n",
        "        The CNN convolutional network with architecture defined above\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # START TODO #############\n",
        "        self.conv1 = nn.Conv2d(3, Nfilters, kernel_size= Ksize , stride=1 , padding=padding)                           \n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(16*16*Nfilters, 10)\n",
        "        # END TODO #############\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: The input tensor with shape (batch_size, feature_dim)\n",
        "            The input to the network will be a minibatch of data\n",
        "                \n",
        "        Returns:\n",
        "            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n",
        "        \"\"\"\n",
        "        # START TODO #############\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        # END TODO #############\n",
        "        return x\n",
        "\n",
        "net = ConvNet2()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet2(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (relu): ReLU()\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=4096, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK29cSA--snr",
        "colab_type": "code",
        "outputId": "c9b9c338-043a-48b5-b3dc-947b7782e99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "# Define a Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(2):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 0.947\n",
            "[1,  2000] loss: 0.799\n",
            "[1,  3000] loss: 0.777\n",
            "[1,  4000] loss: 0.746\n",
            "[1,  5000] loss: 0.711\n",
            "[1,  6000] loss: 0.689\n",
            "[1,  7000] loss: 0.673\n",
            "[1,  8000] loss: 0.677\n",
            "[1,  9000] loss: 0.658\n",
            "[1, 10000] loss: 0.658\n",
            "[1, 11000] loss: 0.660\n",
            "[1, 12000] loss: 0.649\n",
            "[2,  1000] loss: 0.588\n",
            "[2,  2000] loss: 0.590\n",
            "[2,  3000] loss: 0.595\n",
            "[2,  4000] loss: 0.598\n",
            "[2,  5000] loss: 0.609\n",
            "[2,  6000] loss: 0.603\n",
            "[2,  7000] loss: 0.590\n",
            "[2,  8000] loss: 0.615\n",
            "[2,  9000] loss: 0.590\n",
            "[2, 10000] loss: 0.580\n",
            "[2, 11000] loss: 0.579\n",
            "[2, 12000] loss: 0.586\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 61 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VazHlUfi-snt",
        "colab_type": "text"
      },
      "source": [
        "### Changing the parameters of the network - Filter size\n",
        "\n",
        "The filter size in the last network we use 5x5 filters, now use 3x3 filters and describe what happens, is the network more efficient?\n",
        "\n",
        "**Answer**:  \n",
        "**TODO**  \n",
        "The accuracy decreased. So the network is less efficient. Using smaller filter means the receptive field is smaller. This causes the network to capture smaller, more complex features in the image. However, we lose the general overview of the image. This means that the network is less likely to understand the context of the image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIFoqJxG-snu",
        "colab_type": "code",
        "outputId": "6c95378a-67d4-458d-a143-5661023825e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "Nfilters = 16\n",
        "Ksize = 3\n",
        "padding = 1\n",
        "class ConvNet3(nn.Module):\n",
        "    \"\"\"\n",
        "        The CNN convolutional network with architecture defined above\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # START TODO #############\n",
        "        # Define the layers need to build the network\n",
        "        self.conv1 = nn.Conv2d(3, Nfilters, kernel_size= Ksize , stride=1 , padding=padding)                           \n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(4096, 10)\n",
        "        # END TODO #############\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        \n",
        "        Args:\n",
        "            x: The input tensor with shape (batch_size, feature_dim)\n",
        "            The input to the network will be a minibatch of data\n",
        "                \n",
        "        Returns:\n",
        "            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n",
        "        \"\"\"\n",
        "        # START TODO #############\n",
        "        \n",
        "        # Remember to flatten the feature map using x.view\n",
        "        # must have dimentions: N, \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        # END TODO #############\n",
        "        return x\n",
        "\n",
        "net = ConvNet3().to('cuda')\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet3(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=4096, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf0yPN-e-sny",
        "colab_type": "code",
        "outputId": "4243c4e9-8566-49c5-8545-fd22b57a7347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "# Define a Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(2):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs.to('cuda')).cpu()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images.to('cuda')).cpu()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 0.948\n",
            "[1,  2000] loss: 0.818\n",
            "[1,  3000] loss: 0.775\n",
            "[1,  4000] loss: 0.741\n",
            "[1,  5000] loss: 0.719\n",
            "[1,  6000] loss: 0.704\n",
            "[1,  7000] loss: 0.689\n",
            "[1,  8000] loss: 0.684\n",
            "[1,  9000] loss: 0.675\n",
            "[1, 10000] loss: 0.664\n",
            "[1, 11000] loss: 0.651\n",
            "[1, 12000] loss: 0.646\n",
            "[2,  1000] loss: 0.604\n",
            "[2,  2000] loss: 0.595\n",
            "[2,  3000] loss: 0.600\n",
            "[2,  4000] loss: 0.607\n",
            "[2,  5000] loss: 0.599\n",
            "[2,  6000] loss: 0.616\n",
            "[2,  7000] loss: 0.590\n",
            "[2,  8000] loss: 0.598\n",
            "[2,  9000] loss: 0.583\n",
            "[2, 10000] loss: 0.576\n",
            "[2, 11000] loss: 0.594\n",
            "[2, 12000] loss: 0.596\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 53 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qiIIfyG-sn0",
        "colab_type": "text"
      },
      "source": [
        "### Batch normalization\n",
        "\n",
        "Include batch normalization and describe what happens, is the network more efficient?\n",
        "\n",
        "**Answer**:  \n",
        "**TODO**  \n",
        "Yes, the network is more efficient.\n",
        "Batch normalization enables faster and more stable training of a deep neural network. It standardizes the activations from the previous layer. This has the effect of standardizing the learning process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkpYSwjB-sn1",
        "colab_type": "code",
        "outputId": "3cd2bdaa-5a7b-44a2-9cfb-b006a248107f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "Nfilters = 16\n",
        "Ksize = 3\n",
        "padding = 1\n",
        "class ConvNet4(nn.Module):\n",
        "    \"\"\"\n",
        "        The CNN convolutional network with architecture defined above\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # START TODO #############\n",
        "        # Define the layers need to build the network\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, Nfilters, kernel_size= Ksize , stride=1 , padding=padding)  \n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn = nn.BatchNorm2d(num_features=Nfilters)                         \n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(4096, 10)\n",
        "\n",
        "        # END TODO #############\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        \n",
        "        Args:\n",
        "            x: The input tensor with shape (batch_size, feature_dim)\n",
        "            The input to the network will be a minibatch of data\n",
        "                \n",
        "        Returns:\n",
        "            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n",
        "        \"\"\"\n",
        "        # START TODO #############\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.bn(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        # Remember to flatten the feature map using x.view\n",
        "        # must have dimentions: N, \n",
        "        \n",
        "        # END TODO #############\n",
        "        return x\n",
        "\n",
        "net = ConvNet4()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet4(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=4096, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XbkgK7i-sn4",
        "colab_type": "code",
        "outputId": "d6504f6e-7ef8-40be-f27b-e001aa5073dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "# Define a Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(2):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.128\n",
            "[1,  2000] loss: 0.819\n",
            "[1,  3000] loss: 0.765\n",
            "[1,  4000] loss: 0.741\n",
            "[1,  5000] loss: 0.711\n",
            "[1,  6000] loss: 0.700\n",
            "[1,  7000] loss: 0.702\n",
            "[1,  8000] loss: 0.671\n",
            "[1,  9000] loss: 0.688\n",
            "[1, 10000] loss: 0.675\n",
            "[1, 11000] loss: 0.639\n",
            "[1, 12000] loss: 0.647\n",
            "[2,  1000] loss: 0.610\n",
            "[2,  2000] loss: 0.624\n",
            "[2,  3000] loss: 0.619\n",
            "[2,  4000] loss: 0.629\n",
            "[2,  5000] loss: 0.629\n",
            "[2,  6000] loss: 0.632\n",
            "[2,  7000] loss: 0.622\n",
            "[2,  8000] loss: 0.605\n",
            "[2,  9000] loss: 0.627\n",
            "[2, 10000] loss: 0.619\n",
            "[2, 11000] loss: 0.616\n",
            "[2, 12000] loss: 0.608\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 57 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFdOFVwi-sn6",
        "colab_type": "text"
      },
      "source": [
        "Taking into account the previous results, design your own ConvNet to achieve at least 70% of accuracy in max 10 epochs.\n",
        "\n",
        "You can change the network architecture. The most common ConvNet architecture follows the pattern:\n",
        "\n",
        "INPUT -> [[CONV -> RELU]\\*N -> POOL?]\\*M -> [FC -> RELU]\\*K -> FC\n",
        "where the * indicates repetition, and the POOL? indicates an optional pooling layer. Moreover, N >= 0 (and usually N <= 3), M >= 0, K >= 0 (and usually K < 3)\n",
        "\n",
        "But consider that deeper networks will take a lot of time to train.\n",
        "\n",
        "You can also change the loss function and the optimizer.\n",
        "\n",
        "\n",
        "**Describe what you did:**  \n",
        "**TODO**  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWXl6e3p-sn7",
        "colab_type": "code",
        "outputId": "c3c6e395-87b1-41a5-fd2b-d8c515a1f9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "Nfilters = 32\n",
        "Ksize = 3\n",
        "padding = 1\n",
        "class ConvNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # START TODO #############\n",
        "        # Define the layers need to build the network\n",
        "\n",
        "        # ## working with 68%\n",
        "        # self.conv1 = nn.Conv2d(3, 20, kernel_size= Ksize , stride=1 , padding=padding)  \n",
        "        \n",
        "\n",
        "        # self.conv2 = nn.Conv2d(20, 25, kernel_size= Ksize , stride=1 , padding=padding)  \n",
        "        \n",
        "        # self.conv3 = nn.Conv2d(25, Nfilters, kernel_size= Ksize , stride=1 , padding=padding)  \n",
        "        \n",
        "        \n",
        "        # self.pool = nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n",
        "\n",
        "        # self.fc1 = nn.Linear(30752, 1400)\n",
        "        \n",
        "        # self.fc2 = nn.Linear(1400, 128)\n",
        "        \n",
        "        # self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "        # try to improve\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 20, kernel_size= Ksize , stride=1 , padding=padding)  \n",
        "        \n",
        "\n",
        "        self.conv2 = nn.Conv2d(20, 25, kernel_size= Ksize , stride=1 , padding=padding)  \n",
        "        \n",
        "        self.conv3 = nn.Conv2d(25, Nfilters, kernel_size= Ksize , stride=1 , padding=padding)  \n",
        "        \n",
        "        \n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n",
        "\n",
        "        self.fc1 = nn.Linear(30752, 1400)\n",
        "        \n",
        "        self.fc2 = nn.Linear(1400, 150)\n",
        "        \n",
        "        self.fc3 = nn.Linear(150, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "\n",
        "        # END TODO #############\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        \n",
        "        Args:\n",
        "            x: The input tensor with shape (batch_size, feature_dim)\n",
        "            The input to the network will be a minibatch of data\n",
        "                \n",
        "        Returns:\n",
        "            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n",
        "        \"\"\"\n",
        "        # START TODO #############\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        \n",
        "        x = F.relu(self.conv3(x))\n",
        "        \n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        \n",
        "        x = self.fc4(x)\n",
        "        \n",
        "            \n",
        "        # END TODO #############\n",
        "        return x\n",
        "\n",
        "\n",
        "net = ConvNet5().to('cuda')\n",
        "print(net)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet5(\n",
            "  (conv1): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(20, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(25, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=30752, out_features=1400, bias=True)\n",
            "  (fc2): Linear(in_features=1400, out_features=150, bias=True)\n",
            "  (fc3): Linear(in_features=150, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvSkm18j-sn9",
        "colab_type": "code",
        "outputId": "8eca424e-fade-457d-9e26-df57cb753bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(10):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs.to('cuda')).cpu()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images.to('cuda')).cpu()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.140\n",
            "[1,  2000] loss: 1.023\n",
            "[1,  3000] loss: 0.966\n",
            "[1,  4000] loss: 0.896\n",
            "[1,  5000] loss: 0.841\n",
            "[1,  6000] loss: 0.801\n",
            "[1,  7000] loss: 0.776\n",
            "[1,  8000] loss: 0.738\n",
            "[1,  9000] loss: 0.726\n",
            "[1, 10000] loss: 0.701\n",
            "[1, 11000] loss: 0.685\n",
            "[1, 12000] loss: 0.652\n",
            "[2,  1000] loss: 0.628\n",
            "[2,  2000] loss: 0.602\n",
            "[2,  3000] loss: 0.582\n",
            "[2,  4000] loss: 0.591\n",
            "[2,  5000] loss: 0.566\n",
            "[2,  6000] loss: 0.562\n",
            "[2,  7000] loss: 0.551\n",
            "[2,  8000] loss: 0.539\n",
            "[2,  9000] loss: 0.540\n",
            "[2, 10000] loss: 0.519\n",
            "[2, 11000] loss: 0.510\n",
            "[2, 12000] loss: 0.510\n",
            "[3,  1000] loss: 0.404\n",
            "[3,  2000] loss: 0.406\n",
            "[3,  3000] loss: 0.415\n",
            "[3,  4000] loss: 0.398\n",
            "[3,  5000] loss: 0.416\n",
            "[3,  6000] loss: 0.414\n",
            "[3,  7000] loss: 0.405\n",
            "[3,  8000] loss: 0.401\n",
            "[3,  9000] loss: 0.415\n",
            "[3, 10000] loss: 0.401\n",
            "[3, 11000] loss: 0.399\n",
            "[3, 12000] loss: 0.403\n",
            "[4,  1000] loss: 0.239\n",
            "[4,  2000] loss: 0.246\n",
            "[4,  3000] loss: 0.275\n",
            "[4,  4000] loss: 0.255\n",
            "[4,  5000] loss: 0.279\n",
            "[4,  6000] loss: 0.274\n",
            "[4,  7000] loss: 0.269\n",
            "[4,  8000] loss: 0.274\n",
            "[4,  9000] loss: 0.277\n",
            "[4, 10000] loss: 0.276\n",
            "[4, 11000] loss: 0.295\n",
            "[4, 12000] loss: 0.294\n",
            "[5,  1000] loss: 0.117\n",
            "[5,  2000] loss: 0.136\n",
            "[5,  3000] loss: 0.139\n",
            "[5,  4000] loss: 0.136\n",
            "[5,  5000] loss: 0.150\n",
            "[5,  6000] loss: 0.153\n",
            "[5,  7000] loss: 0.166\n",
            "[5,  8000] loss: 0.168\n",
            "[5,  9000] loss: 0.173\n",
            "[5, 10000] loss: 0.172\n",
            "[5, 11000] loss: 0.185\n",
            "[5, 12000] loss: 0.186\n",
            "[6,  1000] loss: 0.063\n",
            "[6,  2000] loss: 0.069\n",
            "[6,  3000] loss: 0.084\n",
            "[6,  4000] loss: 0.084\n",
            "[6,  5000] loss: 0.087\n",
            "[6,  6000] loss: 0.097\n",
            "[6,  7000] loss: 0.099\n",
            "[6,  8000] loss: 0.092\n",
            "[6,  9000] loss: 0.100\n",
            "[6, 10000] loss: 0.097\n",
            "[6, 11000] loss: 0.108\n",
            "[6, 12000] loss: 0.111\n",
            "[7,  1000] loss: 0.036\n",
            "[7,  2000] loss: 0.049\n",
            "[7,  3000] loss: 0.058\n",
            "[7,  4000] loss: 0.063\n",
            "[7,  5000] loss: 0.063\n",
            "[7,  6000] loss: 0.076\n",
            "[7,  7000] loss: 0.057\n",
            "[7,  8000] loss: 0.067\n",
            "[7,  9000] loss: 0.065\n",
            "[7, 10000] loss: 0.071\n",
            "[7, 11000] loss: 0.078\n",
            "[7, 12000] loss: 0.070\n",
            "[8,  1000] loss: 0.030\n",
            "[8,  2000] loss: 0.034\n",
            "[8,  3000] loss: 0.047\n",
            "[8,  4000] loss: 0.042\n",
            "[8,  5000] loss: 0.048\n",
            "[8,  6000] loss: 0.041\n",
            "[8,  7000] loss: 0.044\n",
            "[8,  8000] loss: 0.052\n",
            "[8,  9000] loss: 0.056\n",
            "[8, 10000] loss: 0.053\n",
            "[8, 11000] loss: 0.059\n",
            "[8, 12000] loss: 0.051\n",
            "[9,  1000] loss: 0.019\n",
            "[9,  2000] loss: 0.022\n",
            "[9,  3000] loss: 0.030\n",
            "[9,  4000] loss: 0.031\n",
            "[9,  5000] loss: 0.027\n",
            "[9,  6000] loss: 0.040\n",
            "[9,  7000] loss: 0.042\n",
            "[9,  8000] loss: 0.039\n",
            "[9,  9000] loss: 0.032\n",
            "[9, 10000] loss: 0.036\n",
            "[9, 11000] loss: 0.025\n",
            "[9, 12000] loss: 0.027\n",
            "[10,  1000] loss: 0.016\n",
            "[10,  2000] loss: 0.023\n",
            "[10,  3000] loss: 0.020\n",
            "[10,  4000] loss: 0.035\n",
            "[10,  5000] loss: 0.028\n",
            "[10,  6000] loss: 0.029\n",
            "[10,  7000] loss: 0.030\n",
            "[10,  8000] loss: 0.025\n",
            "[10,  9000] loss: 0.040\n",
            "[10, 10000] loss: 0.030\n",
            "[10, 11000] loss: 0.030\n",
            "[10, 12000] loss: 0.036\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 68 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRgcZJ7ADGuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}